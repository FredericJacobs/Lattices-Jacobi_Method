\documentclass[10pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{color}
\newcommand{\my}[1]{{\color{blue} #1 }}
\setlength{\parindent}{0pt} 
\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 
\title{Comparing the Jacobi Method and LLL lattice reduction algorithms for cryptographic applications}
\date{Fall 2014}
\author{Frederic Jacobs\\ EPFL Bachelor Semester Project\\ frederic.jacobs@epfl.ch}

\renewcommand{\vec}[1]{\mathbf{#1}}

\begin{document}

\maketitle

\section{Introduction}
\subsection{Results and Contributions}

\my{We show the following main results:
\begin{itemize}
\item An analysis of running times depending on factors
\item paper: \cite{originalJacobiMethodLatticeBasisReduction}
\end{itemize}
}
\subsection{Related Work}

\section{Preliminaries}

In this section, we briefly cover the basics of lattices and lattice reduction required in the scope of this report.

\subsection{Euclidean norm}

The Euclidean norm in $\mathbb{R}^n$ is a mapping from $\mathbb{R}^n \to \mathbb{R}$ that assigns the length to any vector of the space.

The mapping is defined for any vector $\vec{b}$ of $\mathbb{R}^n$ with coordinates $b_i$ as follows: 

\[
\|\mathbf{b}\| = \sqrt{b_1^2 + b_2^2 + ... + b_n^2} 
\] 

\my{ which results from??} as a consequence of the Pythagorean theorem.

\subsection{Lattice, also called Euclidean lattice}

Informally, a lattice can be defined as the infinite set of intersection points of an infinite regular grid.

This can be translated into a more formal definition~\cite{SchnorrStanfordNotes}.\newline 
Let $\vec{b_1}, \vec{b_2}, ... \vec{b_n} \in \mathbb{R}^m$ be linearly independent vectors \my{for $n\leq m$}. We call the additive subgroup

\[
L(\vec{b_1}, \vec{b_2},..., \vec{b_n}):= \displaystyle\sum_{i=1}^{n} \vec{b_i} \mathbb{Z}
\] of $\mathbb{R}^m$, a lattice with basis $b_1,b_2,...,b_n$.
A lattice that has $\vec{b_1},\vec{b_2},...,\vec{b_n}$ as basis vectors will be noted in matrix form as $B$ where each vector $\vec{b_i}$ is a column of the matrix.
The \emph{dimension of the lattice} is $dim(L) := rank(L) := n$.

A lattice $L \subseteq \mathbb{R}^m$ is \emph{full dimensional} when $rank(L)=m$. We will work exclusively with full dimensional lattices. In other words, this means that we will work on matrices with $m=n$ linearly independent vectors.

\subsubsection{Equivalent bases}

As this article focuses on lattice basis reduction, it is important to define the equivalence of two bases. Two bases $B_1 , B_2$ are equivalent iff $B_2 = B_1 U$ for a unimodular matrix $U$.

A \emph{unimodular matrix} $U$ is a square integer matrix such that $det(U) = \pm 1$. The right multiplication of a matrix $B$ with a unimodular matrix $U$ \my{?? results can result in a combination following} elementary column operations:
\begin{itemize}
\item Swap two columns
\item Negate the elements of a column
\item Addition of an integer multiple of one column to another
\end{itemize}
\my{A lattice can be described by infinitely many equivalent bases. The dimension, the length of a shortest vector and the volume, as described next, stay invariant under change of basis.}
\subsubsection{Volume}

The volume of a lattice, also known as the determinant of the lattice, is defined as:
\[
\text{vol} \,L = \sqrt{\det(B^T B)}
\] where $B$ is a matrix of the lattice's basis vectors.


The determinant of an unimodular matrix being $\pm 1$, the determinant is an invariant under change of basis. Therefore the volume of a lattice is an invariant under change of basis.

Since we will be studying full dimensional lattices, the number of basis vectors will be equal to the dimension of the lattice and hence, the volume of the lattice can be computed by taking the absolute value of the determinant \my{of the basis matrix}.

For full dimensional lattices, the volume can be computed:
\[
\text{vol} \,L_{\text{full}} = |\det B|
\] where $B$ is a matrix of the linearly independent lattice's basis vectors.


\subsubsection{Minkowski's Convex Body Theorem}

Minkowski's Convex Body Theorem tells us \my{that (grammar)} for a full-rank lattice $L$ of $\mathbb{R}^n$. Let $C$ be a measurable subset of $\mathbb{R}^n$, convex, symmetric with respect to 0, and of measure $> 2^n \text{vol}(L)$. Then $C$ contains at least a nonzero point of the lattice $L$.

A corollary of Minkowski's theorem tells us that there exists for any $n$-dimensional full-rank lattice $L$ a nonzero vector $x$ such that

\[
\lambda_1(L) \leq \sqrt{n} (\det L)^{1/n}
\]

This gives us an upper-bound on \my{ the (better use 'a shortest vector' as there are exponentially many)} shortest vector in a lattice.

\subsubsection{Hermite Normal Form}

The \emph{Hermite Normal Form} representation of a basis matrix $B$, abbreviated \emph{HNF (B)}, is unique. It is the analogue of the reduced echelon form for integer matrices.

A matrix $[a_{ij}] \in M_{m,n} (\mathbb{R})$ with $m \leq n$ is in HNF when\cite{SchnorrStanfordNotes} : {\my You know seem to exchange the role of m and n as m is bounded by n and not the other way around, perhaps one should replace n by a different character, say $n' \geq n$}
\begin{enumerate}
\item $a_{ij} = 0$ for $j > i$, i.e. $A$ is lower triangular. 
\item $a_{ii} > 0$ for $i=1,2,...,m$
\item $0 \leq a_{ij} < a_{ii}$ for $j < i$
\end{enumerate}

\subsection{Special Lattices}

\subsubsection{Random Lattice}

We say that a lattice is a random lattice $L$ \my{ of prime volume $P$} if under HNF form its basis matrix $\vec{B}$ has the following properties:

\begin{itemize}
\item the diagonal has 1 for all it's entries except \my{one position that is set to $P$}a prime number $P$. Hence, the $\det(\vec{B})$ is prime.
\item All \my{column }entries of the matrix under \my{the position that is set to }$P$ are smaller than $P$ in absolute value.
\end{itemize}

\my{Without loss of generality, we hence restrict tests to random lattices of volume $P$ whose basis in HNF form is as follows:
$$\begin{array}{ccccc}
P & & &\\
a_1& 1&   & \\ 
\vdots & &  & \\ 
a_{m-1}& & &1 
\end{array}$$
where $a_i \in \mathbb{Z}/ P\mathbb{Z}$.
}

\subsubsection{Nearly Orthogonal Lattice Bases}

We define a \emph{nearly orthogonal lattice basis} $M$ of dimension $n$ and of bit length $k$ as a $n \times n$ square matrix whose entries are $k$-bits picked at random. These lattices are frequently used for signal processing applications~\cite{originalJacobiMethodLatticeBasisReduction}.

\subsection{Lattice Reduction}

Lattice reduction is the process of finding \my{nearly}\footnote{\my{Did you read nearly orthogonal somewhere? It seems rather strong as a requirement. Better say.. that renders the basis vectors shorter and more orthogonal to one another}} orthogonal vectors from an integer lattice basis. Giving an exact definition of "nearly orthogonal vectors" is tricky and each reduction algorithm is likely to give it's own definition of what it means for a basis to be reduced.

\subsection{Quality of a reduction}

In this section, we will discuss two lattice reduction quality indicators that we will use to compare the result of reduction algorithms.

\subsubsection{Orthogonality defect of a basis}
The \emph{orthogonality defect} of a basis $\vec{b_1},\vec{b_2},...,\vec{b_n}$ of a lattice $L$ is defined by:
\[
    \text{OrthDefect}(L) := \frac{\displaystyle\prod^{n}_{i=1} \|\vec{b_i}\| }{\det(L)}
\]

The orthogonality defect is a quality indicator that we will use to compare the reduction of \emph{the set of basis vectors}. The orthogonality defect can be interpreted geometrically as the product of the basis vector lengths divided by the volume of the lattice. As the basis vectors are getting \my{increasingly? more?} orthogonal during a reduction, the nominator will become smaller. The lower bound of the orthogonality defect is $\det B \ge 1$ \my{why}. If $\det B = 1$, the basis is completely orthogonal. \my{very special case, you refer to Z here, but 2Z is also orthogonal, or did you mean Orthdefect = 1?}

\subsubsection{Hermite factor of a basis}
The \emph{Hermite Factor} of basis vectors $\vec{b_1}, \vec{b_2},...,\vec{b_n}$ of a lattice $L$ is defined by

\[
    \text{HF}(L) := \frac{\|\vec{b_1}\|}{\sqrt[n]{\det(L)}}
\]

The Hermite Factor is a quality indicator that we will use to compare the reduction to a \emph{shortest vector}.

\subsection{Lattice Hard Problems}

Lattices have a few known hard problems that can be used for asymmetric cryptography. We will only cover the most famous one, the Shortest Vector Problem.

\subsubsection{Shortest Vector Problem (SVP)}

Given a basis of a $n$-rank integral lattice $L$, find $\vec{u} \in L$ such that $\|\vec{u}\| = \lambda_1 (L)$

In other words, we are looking for \my{the} shortest vector of the lattice. Finding exactly \my{the} shortest vector of a lattice is known to be NP-hard under randomized reductions\cite{Ajtai:1998:SVP} but the complexity of the deterministic reduction remains open.

\subsection{Gram-Schmidt orthogonalization process}
The \emph{Gram-Schmidt orthogonalization (\emph GSO) process} is a method for orthonormalising a set of vectors. Two vectors are orthonormal if they are both orthogonal and unit vectors. 

The GSO process  can be defined recursively:
\[
\begin{cases}
\vec{b_1}^{\star} = \vec{b_i}  \\
\vec{b_i}^{\star} = \displaystyle\sum^{i-1}_{j=1} \mu_{i,j} \my{\vec{b}}
\end{cases}
\]
\my{where $\mu_{ij} = ..$. Is the set of vectors $b_i^*$ a basis or not? In which case yes?}
\section{The LLL Algorithm}

The LLL algorithm~\cite{lllpaper} was the first polynomial-time reduction algorithm to be introduced outputting a nearly orthogonal basis. LLL is believed to be the most efficient \my{reduction approximation algorithm (better say reduction algorithm hat approximates a shortest vector by at least exponential factors} to date\cite{lllAlgorithm}.

\subsection{$\delta$-LLL Reduced}
We say that a basis is $\delta$-LLL reduced if:
\begin{itemize}
\item $| \mu_{i,j}| \leq \frac{1}{2} \text{, } \forall i > j$
\item Lov치sz's condition: $\forall (\vec{b_i, b_{i+1}})$, we have $(\delta - \mu^2_{i+1,i}) \|\vec{b}^{\star}_{i}\|^2 \leq \| \vec{b}^{\star}_{i+1} \|^2$
\end{itemize}

The Lov치sz conditions assures us that although the Gram-Schmidt vectors can get shorter and shorter, their length in a sorted basis cannot decrease too quickly. \my{Add a small sentence about the first condition if you like.}

Since the LLL-reduction algorithm returns a $\delta$-LLL reduced basis, we can observe the following properties on the basis:
\begin{itemize}
\item $\| \vec{b_1} \| \leq \alpha^{(n-1)/2} \lambda_1 $
\item $\max_i \| \vec{b_1} \| \leq \alpha^{(n-1)/2} \lambda_n $ \my{don't see any i here}
\item $\| b_1 \| \leq \alpha^{(n-1)/4} \det \vec{B}^{1/n}$ \my{why is B bold?}
\end{itemize} 
where the parameter $\alpha = 1/(\delta - \frac{1}{4})$. Since $\delta$ is bounded by $1/4 < \delta < 1$, parameter $\alpha \geq 4/3$.

\subsection{LLL overview}

The LLL algorithm, like many reduction algorithms, can be broken down into two parts: size-reduction and vector swapping. Size-reduction reduces the vectors from the input basis. \my{ until..? to ensure what?} After that, the vectors of the basis are checked against the Lov치sz condition. If a pair of vectors of the basis $(\vec{b_{i}}, \vec{b_{i+1}})$ \my{don't} satisfy the condition, we swap them and go back to the size reduction. my{Procedure 1..}

\begin{algorithm}[H]
\caption{Overview of naive-iterative LLL}
\label{lllAlgorithm}
\begin{algorithmic}[1]
\REQUIRE a basis ($\vec{b_1, ..., b_n}$) of a lattice $L$.
\ENSURE $\delta$-LLL reduced basis $(\vec{b_1, ..., b_n})$

\STATE Size-reduce $(\vec{b_1, ..., b_n})$
\IF{there exists an index $j$ which does not satisfy Lov치sz' condition} \STATE{swap $\vec{b_j}$ and $\vec{b_{j+1}}$, go back to step 1} \ENDIF

\end{algorithmic}
\end{algorithm}

\section{The Jacobi Method}

In 2012, Sanzheng Qiao suggested to use a Jacobi method for lattice basis reduction~\my{\cite{originalJacobiMethodLatticeBasisReduction}.} The idea of the Jacobi method is to \my{use the reduce }larger dimension lattice basis by using the Lagrange algorithm iteratively.

\subsection{Lagrange-reduced basis}

Lagrange was the first to formalize\cite{lagrangeArithmetique} the notion of reduction for rank-two lattices.

Let $L$ be a rank-two lattice of $\mathbb{R}^n$. A basis $(\vec{b_1}, \vec{b_2})$ of $L$ is said to be Lagrange-reduced if and only if $\|\vec{b_1}\| \leq \|\vec{b_2}\|$ and $| \langle \vec{b_1} \vec{b_2} \rangle | \leq \| \vec{b_1} \|^2 / 2 $. 

\subsection{Lagrange algorithm}
\label{sec:lagrangeAlgorithm}

Lagrange's algorithm solves the lattice reduction problem exactly for lattice bases of dimension 2. It is a variant of Euler's centered algorithm that is used to compute the greatest common \my{divider} of two numbers. GCD can be thought to be a one dimension reduction problem. The GCD is the first minimum of the lattice $n \mathbb{Z} + m \mathbb{Z}$ spanned by $n$ and $m$.

\begin{algorithm}[H]
\caption{Lagrange algorithm}
\label{lagrangeAlgorithm}
\begin{algorithmic}[1]
\REQUIRE a basis ($\vec{b_1, b_2}$) of a lattice $L$ of dimension 2.
\ENSURE a Lagrange reduced reduced basis $(\vec{b_1, b_2})$

\IF{$\|\vec{b_1} \| < \| \vec{b_2} \| $} \STATE{swap $\vec{b_1}$ and $\vec{b_2}$} \ENDIF
\REPEAT \STATE{
$q = \lfloor \frac{ \langle \vec{b_1} \vec{b_2} \rangle }{\| v\|^2} \rceil $\\
$r \leftarrow \vec{b_1} - q \vec{b_2}$\\
$\vec{b_1} \leftarrow \vec{b_2}$\\
$\vec{b_2} \leftarrow r$
} \UNTIL{$\| u\| \leq \| v \| $}

\end{algorithmic}
\end{algorithm}
\my{some v's and u's at three positions seem stroll around in the above algorithm}

\subsection{The Generic Jacobi Method}
We call \emph{generic Jacobi method} the algorithm that simply applies the Lagrange algorithm to every pair of the basis vectors for an $n$ dimensional lattice. This method was introduced~my{\cite{originalJacobiMethodLatticeBasisReduction}} by Qiao in 2012 claiming experimental results to be considerably faster than LLL. We haven't been able to reproduce the claimed running times of this algorithm.

From the definition of a Lagrange-reduced basis, we can define a Jacobi-reduced basis such as:
\begin{align}
\langle \vec{b_i} \vec{b_j} \rangle &\leq \frac{1}{2} \| \vec{b_i}\|^2 \text{ (for all $1 \leq i < j \leq n$)}.\\
\| \vec{b_i}\| &\leq \|\vec{b_j} \| \text{ (for all $1 \leq i < j \leq n$)}
\end{align}


\begin{algorithm}[H]
\caption{Generic Jacobi Method}
\label{genericJacobiMethod}
\begin{algorithmic}
\REQUIRE a basis matrix ($\vec{b_1,...,b_n}$)
\ENSURE a generic-Jacobi reduced basis ($\vec{b_1,...,b_n}$)

\WHILE{not all pairs ($\vec{b_i,b_j}$) satisfy both generic-Jacobi reduction conditions}
    \FOR{$i = 1$ \TO $n-1$ }
        \FOR{$j = i + 1$ \TO $n$ }
            \STATE {$[\vec{b_i, b_j}]$} = \emph{Lagrange$(\vec{b_i, b_j})$}
        \ENDFOR
    \ENDFOR
\ENDWHILE

\end{algorithmic}
\end{algorithm}

\subsection{A Jacobi Variant}

In this part, we present the Jacobi-variant that was used for running our benchmarking.

\subsubsection{$\omega$-Lagrange-reduced conditions}
An $\omega$ is introduced to \my{...?}
There are two conditions for a basis to be $\omega$-Lagrange-reduced.
\[
\begin{cases}
\lfloor \vec{G}_{ij} / \vec{G}_{ss} \rceil \leq 1, \\
\omega^2 \vec{G}_{ll} \leq \vec{G}_{ii} + \vec{G}_{jj} - 2|\vec{G}_{ij}|

\end{cases}
\]  where $1/\sqrt{3} \leq \omega < 1$. \my{introduce G, do you really want to use $G_{ij}$ and not $g_{ij}$, is it a vector, a matirx or a number?, stay consequent with notation}

\subsubsection{Size-reduction Lagrange}
The size-reduction algorithm is a variant of the Lagrange algorithm that we presented in section \ref{sec:lagrangeAlgorithm}. The reduction is only performed if the $\omega$-Lagrange-reduced conditions are not satisfied. In which case, both the unimodular reduction matrix $\vec{Z}$ and the Gram matrix $\vec{G}$ are updated by applying the Lagrange reduction to the longest of the basis vectors at index $i$ or $j$.

\begin{algorithm}[H]
\caption{LagrangeIT}
\label{customLagrangeIT}
\begin{algorithmic}
\REQUIRE The matrices $\vec{G, Z}$, a pair of indices $(i,j):i<j$ and a parameter $\omega$
\ENSURE  Updated $\vec{G, Z}$ where one Lagrange iteration was performed on the $i$th and $j$th basis vectors. 

\STATE {$s \leftarrow i$}
\STATE {$l \leftarrow j$}

\IF{$\vec{G}_{ii} > \vec{G}_{jj}$}
    \STATE {$s \leftarrow j; l \leftarrow i$}
\ENDIF

\STATE{$q \leftarrow \lfloor \frac {\vec{G}_{ii}}{\vec{G}_{ss}} \rceil $ \my{better work with s and l as indices}}

\IF {Verify both $\omega$-Lagrange-reduced conditions}

\STATE{$\vec{Z_l} -= q* Z_s $}
\STATE{$\vec{G_l} -= q* G_s $}

\FOR{every column $k$ of $\vec{G}$}
\STATE{$\vec{G}_{lk} = \vec{G}_{kl} $}
\ENDFOR

\STATE{$\vec{G}_{ll} -= q* \vec{G}_{ls} $}
\ENDIF

\end{algorithmic}
\end{algorithm}
\my{Please make sure that there is no mistake above. I will check it later.}

\subsection{Jacobi-variant algorithm}
This Jacobi-variant performs Lagrange reductions iteratively on the Gram matrix until no more reduction is done. That is when both $\omega$-reduced Lagrange conditions are satisfied. 

\begin{algorithm}[H]
\caption{Jacobi-Variant Reduction}
\label{jacobiVariantReduction}
\begin{algorithmic}
\REQUIRE a basis matrix ($\vec{B = b_1,...,b_n}$) and $\omega$
\ENSURE a reduced basis ($\vec{b_1,...,b_n}$) where each pair of vectors is $\omega$-Lagrange reduced
\STATE{$\vec{G} = \langle \vec{B} \rangle$, $\vec{Z} = I_n$}
\WHILE{LagrangeIT method reduced the basis vectors}
    \FOR{$i = 1$ \TO $n-1$ }
        \FOR{$j = i + 1$ \TO $n$ }
            \STATE {$[\vec{G,Z}]$} = \emph{LagrangeIT$(\vec{G,Z},i,j,\omega)$}
        \ENDFOR
    \ENDFOR
\ENDWHILE

\end{algorithmic}
\end{algorithm}


\subsection{Our Implementation}

We implemented this Jacobi algorithm in C++ with the \emph{newNTL} library. The FPLLL implementation was used for LLL reductions.

\subsection{Experimental Results}

We notice that the Jacobi algorithm does reduce the bases further than LLL. \my{grammar.. } This comes that the LLL reduction definition is more.
The Jacobi method requires the vectors to be \my{...?}

We show two examples of bases that are LLL-reduced but not $\omega > 0.9$ Jacobi reduced.
\begin{enumerate}

\item \[
\vec{B} = \begin{bmatrix}
  \vec{b_1} \\
  \vec{b_2} \\
  \vec{b_3}
 \end{bmatrix} = \begin{bmatrix}
  0 & 2 & 0 \\
  0 & 1 & 2 \\
  2 & 0 & 0
 \end{bmatrix}
\]


\item \[
\vec{B} = \begin{bmatrix}
  \vec{b_1} \\
  \vec{b_2} \\
  \vec{b_3}
 \end{bmatrix} = \begin{bmatrix}
  4/3 & 0 & 0 \\
  2/3 & \sqrt{4/3} & 0 \\
  2/3 & \sqrt{4/3}/2 & 1
 \end{bmatrix}
\]
\my{In the first case, LLL will stop while Jacobi does exchange $B-2$ and $b_3$. In the second case...}
The Jacobi Method will find a shortest vector by subtracting $\vec{b_1}-\vec{b_2}$, while LLL won't be able to reduce further.
\end{enumerate}


\section{Conclusions}

\bibliographystyle{alpha}
\bibliography{references}

\section{Credits}

\end{document}